任务型对话系统中的自然语言生成（NLG）模块，其输入是pipeline方法中上游模块DM模块的输出，即语义表示（meaning represetation，MR），输出是自然语言回复。NLG的主要任务是将DM模块输出的含义表示经一系列变换，转换成句法合法、语义准确的自然语言形式的回复。

NLG需要关注的四点：充分性 - 流畅性 - 可读性 - 变化性

## NLG神经网络方法

基于神经网络的NLG方法种类繁多，解决的问题也不尽相同，我们按照年份进行一个总结。

### 2015

#### 基于RNN类的NLG方法

Tsung-Hsien Wen等人的工作将神经网络的方法首次应用到任务型对话NLG任务，结合了神经网络和语言模型进行语言生成。这些方法通常分为语言生成（generation）模块和重排序（reranking）模块。

**语言生成模块：**任务是将输入的语义表示转化成多个可能的去词化（delexicalisation）的语句，去词化的语句中只有槽位，不存在槽值。

此模块输入含义表示，应用RNN及其衍生的神经网络结构，如LSTM、Attention-based RNN Encoder-Decoder等结构，使用排序（ranking）的方式输出多个可能的句子。训练时，输入的单词序列可以使用one-hot编码，也可以使用GloVe词向量等更高效的表示方法。使用one-hot编码的含义表示对生成的句子进行语义信息控制（semantic control），具体的做法是使用门控机制，根据每个槽位第一次生成的位置进行对应的衰减，也可以设置可训练的参数一起训练。

**重排序模块：**任务是将生成的去词化语句进行重新打分，选出最好的语句并输出语句。

此模块的输入是语言生成模块生成的多个可能的去词化语句，输出是这个语句的评分。通常使用CNN或者Backward RNN进行重排序。当使用CNN时，一般的做法是经过卷积层、最大池化层和全联接层，生成表示自然语句的one-hot向量，与参考向量一起计算评分。而Backward RNN的方法会将输入的去词化语句逆序输入到RNN模块，得到逆序语句的表示，用于生成句子的评分。

**优缺点：**开创了在任务型对话NLG使用神经网络方法的先河，同时使用了语言模型和神经网络方法，生成的语句具有多样性，且减少了人工工作。缺点在于，使用语义控制的one-hot编码含有的信息较少，设计较为简单；生成语句的标准太简单，搜索空间小，有时无法得到较好的结果；模型无法应对二值化或者无法去词化的情况。

### 2016

#### 基于迁移学习的NLG方法

迁移学习的任务是从数据充足的领域中获取信息，使模型在缺乏数据的新领域达到更好的效果。通常迁移学习流程是：在源领域（去词化后）的数据上训练一个model，然后使用early-stop和更小的初始学习率，将这个model在目标领域数据上去训练。但是这个流程的问题在于，相似的slot-value参数难以共享，而且目标领域有很多的独有的slot，只能重新学习。所以Tsung-Hsien Wen等人提出了一种数据伪造方法来从源领域合成目标领域数据。

**数据构造：**

输入是源领域和目标领域的训练样例，即语义表示和对应的语句（Dialogue act，slot-value pairs，sentence）。首先根据相似性，将源领域和目标领域的slot分类。在这里，分成了三类，informable，requestable，binary，且类别之间是互斥的。然后对所有的slot和value进行去词化处理。对于源领域样例中的所有slot，随机的用一个目标领域中与其分类相同的slot替换，形成一个新的样例，进而形成一个伪造的数据集。

**生成模型：**

在伪造的数据集上基于SC-LSTM结构训练一个模型，然后在真实的目标领域数据集上微调，调整优化SC-LSTM参数在目标领域上获得更好的效果。

**优缺点：**提出了一个迁移学习的数据伪造法，用类似于数据增广的方式提升了模型在缺乏数据的领域的效果。缺点在于随机替换的形成的句子，在语法上可能是正确的，但在语义上很可能不是一个正确的句子，对于语言模型部分的学习可能帮助不大。

#### 基于seq2seq类的NLG方法

传统的NLG方法包括语句规划（sentence planning）和表层生成（surface realization ）两个部分。语句规划是生成一个句子结构树
表层生成则是确定单词形式，将结构化为一个自然语言字符串。Ondrej Dusek和Filip Jurcicek等人沿用了传统的框架，引入了端到端（seq2seq）的生成结构将输入的语义表示直接生成自然语言。实验证明，这种直接方法更有利。

**语句规划模块：**任务是将语义表示转化为深层语法树（deep syntax tree）。

该模块输入的语义表示包括语言行为（Dialogue act）和槽位槽值对（slot-value pairs），语句规划模块将其转化为深层语法树，再写成语义三元组拼接的序列作为生成模块的输入。

**自然语言生成模块：**任务是将语义三元组序列转化为自然语言。

该模块整体采用端到端结构，包含一个encoder和decoder。encoder使用LSTM对三元组序列进行编码，生成中间表示。也可以在encoder中加入对用户话语的编码，因为用户语言也含有语义和语法信息。decoder使用LSTM结构和注意力（attention）机制进行解码，使用句子级别的attention分数乘上每个词的加权和，生成自然语言。

**重排序模块：**任务是输入自然语言得到分数。

该模块采用汉明距离对输入的自然语言进行打分。首先会将输入的DA转换为一个multi-one-hot向量，然后将之前的输出的自然语言再依次经过自然语言生成模块的encoder层，线性层，sigmoid激活得到一个这个生成结果的multi-one-hot向量，对比DA的向量来计算汉明距离得到分数。如果在自然语言生成模块引入了用户话语的编码，需要增加一个重排序标准针对与context的n-gram相似度。

**优缺点：**使用了端到端的结构进行自然语言生成，相比于RNN-based的方法生成的语句效果更好。缺点在于进行三元组输入的形式过于简单，重排序的设计也比较粗糙。

### 2017

#### The E2E dataset：针对于端到端生成的更复杂的数据集

MR：只有slot pair，没有DA

特点：每个MR的reference特别多

baseline model：seq2seq+attn



#### 基于Aggregation的NLG方法

该方法通常分为自然语言生成模块和重排序模块。整体的框架延续Tsung-Hsien Wen等人提出的基于encoder-decoder框架。但是自然语言生成模块在encoder，decoder两个部分的基础上加了聚合器（aggregator）部分，用于提高中间表示的语义和语法信息。

**自然语言生成模块：**任务是将输入的语义表示转化成多个可能的语句。

该模块基于Attention-based RNN Encoder-Decoder结构。首先将槽类型（slot type）和槽值（slot value）的嵌入拼接作为这个槽类型-槽值对的表示，输入encoder得到中间表示，然后用上一个时刻输出的隐层来计算注意力得分。

聚合器（Aggregator）由对齐器（Aligner）和精炼器（Refiner）两部分组成。对齐器（Aligner）的任务是将encoder输出的结果和DA embedding拼接作为新的输出。精炼器（Refinder）的任务是将对齐器（Aligner）处理后的新的结果与当前时间步的输入单词进行处理，以得到信息更加丰富的中间语义表示。具体的实现方法分为基于注意力的方法和基于门控的方法 。

decoder的任务是由中间语义生成语句。该部分使用RNN类的结构和束搜索（beam search）生成多个可能的语句。

**重排序模块：**任务是将生成的去词化语句进行重新打分。

该模块和基于RNN类方法的重排序模块完全一致。

**优缺点：**该方法通过引入聚合器（aggregator）丰富了encoder生成的表示的语义信息，很大程度地提升了生成语句的质量。缺点在于聚合器（Aggregator）的精炼器（Refiner）部分种类繁杂，不具有普适性。



### 2018



### 2019

#### 基于Hierarchy的NLG方法

该方法关注了输入语义表示的结构信息，使用一个图式的结构来掌握语义信息，同时联合Transformer的语言生成结构，达到了很好的效果。

**数据处理：**任务是将输入的语义表示转化为无向图结构。

该模块使用了图结构描述输入的语义表示。由于不同的语义粒度，语义表示天生具有通用的分层属性，每一个语义表示都可以被看作是从根节点到叶子节点的路径，经典的树结构可以抓住不同语义表示之间的亲属关系，然而这种方法阻碍了不同槽（slot）之间的交叉领域的迁移以及不同值之间的交叉值迁移，导致模型会得到一个及其臃肿的树结构表示，因此该模块通过合并具有相同语义的节点来建立一个无向图网络。由于这种组合性，基于图表示可以对为新的（或不太常见的）情况进行迁移。

**自然语言生成模块：**任务是将图结构的语义表示转化为自然语言。

该模块基于Transformer的结构，包含encoder，dialog act predictor和Hierarchiacal DSA三个模块。

encoder的任务是使用神经网络对历史对话进行编码，具体的实现方法可以是CNN结构也可以是RNN模块。dialog act predictor接受encoder的输出和图结构的语义表示共同预测dialog act的分层表示。在decoder部分使用 Hierarchiacal DSA，内部是Disentangled Self Attention机制，并且将Transformer进行结构化处理，底层的Transformer处理底层的slot相关的语义信息，高层的Transformer处理高层的domain相关的语义信息。同时不同于标准的Transformer结构将不同head的输出拼接成一个向量，decoder使用分层表示进行控制，激活特定的head，从而更好地控制信息传递的方向。

**优缺点：**该方法关注了前人没关注的结构信息，使用图结构的语义信息进行生成控制，提升了模型的迁移性。同时结合了Transformer的多抽头结构来进行attention关注，获得了较好的语言生成效果。缺点在于。。。？？？



#### NLG-LM架构的方法

该方法同时训练NLG和语言模型（LM），在保证语义正确的基础上生成流畅的语言。

Encoder是简单GRU，输入序列是去词化的token序列。而Decoder采用了双GRU设计，针对NLG的decoder-GRU会对encoder结果进行attention，而针对于语言模型的decoder-GRU并不对encoder的结果进行解码，而只输入自然语言进行训练。输出为生成去词化的句子，最后通过reversal lexicalization生成自然语言。

**优缺点：**该方法融合了语言模型，可以生成高质量更自然的回复。缺点在于。。。？？？



#### NLG的MRs增强方法

NLG的幻觉现象很多，可能的原因之一是data端不合理，即MR和输出文本缺少显式的对齐信息。该方法通过重建数据集增强MRs来提高模型表现。通常的做法是首先建立NLU对MR进行分析学习，使用NLU对现有的MRs进行重建，再使用self-training的方式，保留confidence比较高的MRs直到不再变化。

**MRs增强过程：**任务是通过encoder-decoder框架重建数据集。

该过程首先将每个槽位-槽值对（slot-value pair）展开作为一串短的序列，使用self-attentive encoder，依次通过BiLSTM和self-attention得到这个槽位-槽值对的表示。同样的，将对应文本通过BiLSTM得到句子的每个token的隐层表示。然后使用pair的表示对句子中的每个token做attention，加权求和得到句子的表示。

decoder部分的任务是slot-value pair和句子表示进行打分。这个过程会穷举所有的槽类型（slot type）和槽值（slot value）的组合进行打分，以获取每种槽类型下最可能的结果。

**Self-training训练方式：**整个过程使用了self-training进行更新迭代。首先会对源语义表示的一段进行训练作为热身（warm up），然后使用它对所有的可能打分，用于控制语义表示。每轮的打分结果都包括对pairs和整个语义表示的打分。采用阈值控制，去除低分的pairs和包含太多pairs的语义表示。迭代到上限或语义表示不再变化为止。

**优缺点：**该方法提出了解决数据端问题的合理解决方式。缺点在于忽略了语义表示中语义级别的信息，这些信息需要外部知识的支持；该方法假设了噪声在语义表示中是中等规模，在某些数据集使用受限。



### 2020



